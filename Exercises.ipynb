{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2b9194f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8406b6b2-5c38-42e1-bdd2-c0ebf5fc0307",
   "metadata": {},
   "source": [
    "## PyTorch exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466028e0-871c-4c72-868b-0b8439e659be",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "\n",
    "1. Make a tensor of size (2, 17)\n",
    "2. Make a torch.FloatTensor of size (3, 1)\n",
    "3. Make a torch.LongTensor of size (5, 2, 1)\n",
    "  - fill the entire tensor with 7s\n",
    "4. Make a torch.ByteTensor of size (5,)\n",
    "  - fill the middle 3 indices with ones such that it records [0, 1, 1, 1, 0]\n",
    "5. Perform a matrix multiplication of two tensors of size (2, 4) and (4, 2). Then do it in-place.\n",
    "6. Do element-wise multiplication of two randomly filled $(n_1,n_2,n_3)$ tensors. Then store the result in an Numpy array.\n",
    "\n",
    "### Forward-prop/backward-prop\n",
    "1. Create a Tensor that `requires_grad` of size (5, 5).\n",
    "2. Sum the values in the Tensor.\n",
    "3. Multiply the tensor by 2 and assign the result to a new python variable (i.e. `x = result`)\n",
    "4. Sum the variable's elements and assign to a new python variable\n",
    "5. Print the gradients of all the variables\n",
    "6. Now perform a backward pass on the last variable (NOTE: for each new python variable that you define, call `.retain_grad()`)\n",
    "7. Print all gradients again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766d7d57-678c-4bf1-8754-06d1383e9884",
   "metadata": {},
   "source": [
    "### Deep-forward NNs\n",
    "1. Look at Lab 3. In Exercise 12 there, you had to build an $L$-layer neural network with the following structure: *[LINEAR -> RELU]$\\times$(L-1) -> LINEAR -> SIGMOID*. Reimplement the manual code in PyTorch.\n",
    "2. Compare test accuracy using different optimizers: SGD, Adam, Momentum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e077de",
   "metadata": {},
   "source": [
    "#### Tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ecec7d30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Tensor of size (2, 17):\n",
      " tensor([[-3.6371e-38,  1.9408e-42,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00],\n",
      "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "          0.0000e+00,  0.0000e+00]])\n",
      "Shape: torch.Size([2, 17])\n"
     ]
    }
   ],
   "source": [
    "# 1. Make a tensor of size (2, 17)\n",
    "tensor_1 = torch.empty(2, 17)\n",
    "print(\"1. Tensor of size (2, 17):\\n\", tensor_1)\n",
    "print(\"Shape:\", tensor_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "692efac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. FloatTensor of size (3, 1):\n",
      " tensor([[-3.5455e-38],\n",
      "        [ 1.9408e-42],\n",
      "        [ 1.3090e+00]])\n",
      "Shape: torch.Size([3, 1])\n",
      "Type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# 2. Make a torch.FloatTensor of size (3, 1)\n",
    "tensor_2 = torch.FloatTensor(3, 1)\n",
    "print(\"\\n2. FloatTensor of size (3, 1):\\n\", tensor_2)\n",
    "print(\"Shape:\", tensor_2.shape)\n",
    "print(\"Type:\", tensor_2.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "49601c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. LongTensor of size (5, 2, 1) filled with 7s:\n",
      " tensor([[[7],\n",
      "         [7]],\n",
      "\n",
      "        [[7],\n",
      "         [7]],\n",
      "\n",
      "        [[7],\n",
      "         [7]],\n",
      "\n",
      "        [[7],\n",
      "         [7]],\n",
      "\n",
      "        [[7],\n",
      "         [7]]])\n",
      "Shape: torch.Size([5, 2, 1])\n",
      "Type: torch.int64\n"
     ]
    }
   ],
   "source": [
    "# 3. Make a torch.LongTensor of size (5, 2, 1)\n",
    "#    fill the entire tensor with 7s\n",
    "tensor_3 = torch.LongTensor(5, 2, 1)\n",
    "tensor_3.fill_(7)\n",
    "print(\"\\n3. LongTensor of size (5, 2, 1) filled with 7s:\\n\", tensor_3)\n",
    "print(\"Shape:\", tensor_3.shape)\n",
    "print(\"Type:\", tensor_3.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "803024ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. ByteTensor [0, 1, 1, 1, 0]:\n",
      " tensor([0, 1, 1, 1, 0], dtype=torch.uint8)\n",
      "Shape: torch.Size([5])\n",
      "Type: torch.uint8\n"
     ]
    }
   ],
   "source": [
    "# 4. Make a torch.ByteTensor of size (5,)\n",
    "#    fill the middle 3 indices with ones such that it records [0, 1, 1, 1, 0]\n",
    "tensor_4 = torch.ByteTensor(5).zero_()\n",
    "tensor_4[1:4] = 1\n",
    "print(\"\\n4. ByteTensor [0, 1, 1, 1, 0]:\\n\", tensor_4)\n",
    "print(\"Shape:\", tensor_4.shape)\n",
    "print(\"Type:\", tensor_4.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bdda0a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Matrix multiplication result (2, 2):\n",
      " tensor([[ 0.7433,  0.2292],\n",
      "        [-3.1451,  5.0505]])\n",
      "Shape: torch.Size([2, 2])\n",
      "5. Matrix multiplication result (using out=, effectively in-place for output_tensor):\n",
      " tensor([[ 0.7433,  0.2292],\n",
      "        [-3.1451,  5.0505]])\n"
     ]
    }
   ],
   "source": [
    "# 5. Perform a matrix multiplication of two tensors of size (2, 4) and (4, 2). \n",
    "#    Then do it in-place.\n",
    "tensor_5a = torch.randn(2, 4)\n",
    "tensor_5b = torch.randn(4, 2)\n",
    "result_5 = torch.matmul(tensor_5a, tensor_5b)\n",
    "print(\"\\n5. Matrix multiplication result (2, 2):\\n\", result_5)\n",
    "print(\"Shape:\", result_5.shape)\n",
    "\n",
    "output_tensor = torch.empty(2,2)\n",
    "torch.matmul(tensor_5a, tensor_5b, out=output_tensor)\n",
    "print(\"5. Matrix multiplication result (using out=, effectively in-place for output_tensor):\\n\", output_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ed174c40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "6. Element-wise multiplication of two (2,3,4) tensors (first few elements):\n",
      " tensor([ 0.2107, -1.2684,  0.1010,  0.0406])\n",
      "Result stored in NumPy array (first few elements):\n",
      " [ 0.21072632 -1.268359    0.10100806  0.0405582 ]\n",
      "Type of result_6_numpy: <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# 6. Do element-wise multiplication of two randomly filled (n1,n2,n3) tensors.\n",
    "#    Then store the result in an Numpy array.\n",
    "n1, n2, n3 = 2, 3, 4\n",
    "tensor_6a = torch.randn(n1, n2, n3)\n",
    "tensor_6b = torch.randn(n1, n2, n3)\n",
    "result_6_torch = tensor_6a * tensor_6b \n",
    "print(f\"\\n6. Element-wise multiplication of two ({n1},{n2},{n3}) tensors (first few elements):\\n\", result_6_torch[0,0,:])\n",
    "\n",
    "result_6_numpy = result_6_torch.numpy()\n",
    "print(\"Result stored in NumPy array (first few elements):\\n\", result_6_numpy[0,0,:])\n",
    "print(\"Type of result_6_numpy:\", type(result_6_numpy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f019460d",
   "metadata": {},
   "source": [
    "#### Forward-prop/backward-prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca984609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Tensor with requires_grad=True:\n",
      " tensor([[-1.2291,  1.7068,  1.3085,  0.6111, -0.0413],\n",
      "        [ 0.7736,  0.1475,  0.4648,  2.0190,  1.4789],\n",
      "        [-0.4838, -0.7430,  0.6678,  1.4539, -1.2124],\n",
      "        [-1.7750, -1.1107, -1.0405, -0.8410,  2.0359],\n",
      "        [ 0.5419, -0.1954, -0.2859, -1.3401,  0.1933]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a Tensor that `requires_grad` of size (5, 5).\n",
    "tensor_fp_1 = torch.randn(5, 5, requires_grad=True)\n",
    "print(\"1. Tensor with requires_grad=True:\\n\", tensor_fp_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "41054584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. Sum of values:\n",
      " tensor(3.1049, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 2. Sum the values in the Tensor.\n",
    "sum_fp_2 = torch.sum(tensor_fp_1)\n",
    "print(\"\\n2. Sum of values:\\n\", sum_fp_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9ac07ee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3. Tensor multiplied by 2 (x):\n",
      " tensor([[-2.4582,  3.4136,  2.6169,  1.2222, -0.0826],\n",
      "        [ 1.5472,  0.2951,  0.9295,  4.0380,  2.9577],\n",
      "        [-0.9677, -1.4860,  1.3356,  2.9078, -2.4247],\n",
      "        [-3.5500, -2.2213, -2.0810, -1.6820,  4.0719],\n",
      "        [ 1.0838, -0.3908, -0.5718, -2.6802,  0.3867]], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 3. Multiply the tensor by 2 and assign the result to a new python variable (i.e. `x = result`)\n",
    "x_fp_3 = tensor_fp_1 * 2\n",
    "x_fp_3.retain_grad() \n",
    "print(\"\\n3. Tensor multiplied by 2 (x):\\n\", x_fp_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3d46ecde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "4. Sum of x's elements:\n",
      " tensor(6.2098, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 4. Sum the variable's elements and assign to a new python variable\n",
    "sum_x_fp_4 = x_fp_3.sum()\n",
    "sum_x_fp_4.retain_grad()\n",
    "print(\"\\n4. Sum of x's elements:\\n\", sum_x_fp_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3da56c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "5. Gradients before backward pass:\n",
      "   Gradient of tensor_fp_1: None\n",
      "   Gradient of sum_fp_2 (leaf w.r.t this operation, but not in the new graph): None\n",
      "   Gradient of x_fp_3: None\n",
      "   Gradient of sum_x_fp_4: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15132\\428405972.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(\"   Gradient of sum_fp_2 (leaf w.r.t this operation, but not in the new graph):\", sum_fp_2.grad)\n"
     ]
    }
   ],
   "source": [
    "# 5. Print the gradients of all the variables\n",
    "print(\"\\n5. Gradients before backward pass:\")\n",
    "print(\"   Gradient of tensor_fp_1:\", tensor_fp_1.grad)\n",
    "print(\"   Gradient of sum_fp_2 (leaf w.r.t this operation, but not in the new graph):\", sum_fp_2.grad) \n",
    "print(\"   Gradient of x_fp_3:\", x_fp_3.grad)\n",
    "print(\"   Gradient of sum_x_fp_4:\", sum_x_fp_4.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "49941afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Now perform a backward pass on the last variable\n",
    "sum_x_fp_4.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee44af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "7. Gradients after backward pass:\n",
      "   Gradient of tensor_fp_1:\n",
      " tensor([[2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2., 2.]])\n",
      "   Gradient of sum_fp_2 (still None as it's not in the current graph path): None\n",
      "   Gradient of x_fp_3:\n",
      " tensor([[1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1., 1.]])\n",
      "   Gradient of sum_x_fp_4:\n",
      " tensor(1.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_15132\\581574369.py:4: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\build\\aten\\src\\ATen/core/TensorBody.h:494.)\n",
      "  print(\"   Gradient of sum_fp_2 (still None as it's not in the current graph path):\", sum_fp_2.grad)\n"
     ]
    }
   ],
   "source": [
    "# 7. Print all gradients again\n",
    "print(\"\\n7. Gradients after backward pass:\")\n",
    "print(\"   Gradient of tensor_fp_1:\\n\", tensor_fp_1.grad)\n",
    "print(\"   Gradient of sum_fp_2 (still None as it's not in the current graph path):\", sum_fp_2.grad)\n",
    "print(\"   Gradient of x_fp_3:\\n\", x_fp_3.grad) \n",
    "print(\"   Gradient of sum_x_fp_4:\\n\", sum_x_fp_4.grad) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9dabd4d",
   "metadata": {},
   "source": [
    "#### Deep-forward NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42864b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "\n",
    "class LLayerNetwork(nn.Module):\n",
    "    def __init__(self, layer_dims):\n",
    "        super(LLayerNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layer_dims = layer_dims\n",
    "        num_layers = len(layer_dims)\n",
    "\n",
    "        for i in range(num_layers - 2): \n",
    "            self.layers.append(nn.Linear(layer_dims[i], layer_dims[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        \n",
    "        self.layers.append(nn.Linear(layer_dims[num_layers - 2], layer_dims[num_layers - 1]))\n",
    "        self.layers.append(nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acab338e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing Optimizers:\n",
      "\n",
      "Training with SGD...\n",
      "Test Accuracy with SGD: 48.00%\n",
      "\n",
      "Training with Adam...\n",
      "Test Accuracy with Adam: 46.00%\n",
      "\n",
      "Training with SGD_Momentum...\n",
      "Test Accuracy with SGD_Momentum: 58.00%\n",
      "\n",
      "--- Optimizer Comparison Results ---\n",
      "SGD: 48.00% Test Accuracy\n",
      "Adam: 46.00% Test Accuracy\n",
      "SGD_Momentum: 58.00% Test Accuracy\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset \n",
    "\n",
    "class LLayerNetwork(nn.Module):\n",
    "    def __init__(self, layer_dims):\n",
    "        super(LLayerNetwork, self).__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        num_layers = len(layer_dims)\n",
    "        for i in range(num_layers - 2):\n",
    "            self.layers.append(nn.Linear(layer_dims[i], layer_dims[i+1]))\n",
    "            self.layers.append(nn.ReLU())\n",
    "        self.layers.append(nn.Linear(layer_dims[num_layers - 2], layer_dims[num_layers - 1]))\n",
    "        self.layers.append(nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "input_size = 20\n",
    "hidden1_size = 15\n",
    "hidden2_size = 10\n",
    "output_size = 1 \n",
    "layer_dimensions = [input_size, hidden1_size, hidden2_size, output_size]\n",
    "\n",
    "X_train_np = np.random.rand(100, input_size).astype(np.float32)\n",
    "y_train_np = np.random.randint(0, 2, (100, 1)).astype(np.float32)\n",
    "X_test_np = np.random.rand(50, input_size).astype(np.float32)\n",
    "y_test_np = np.random.randint(0, 2, (50, 1)).astype(np.float32)\n",
    "\n",
    "X_train = torch.from_numpy(X_train_np)\n",
    "y_train = torch.from_numpy(y_train_np)\n",
    "X_test = torch.from_numpy(X_test_np)\n",
    "y_test = torch.from_numpy(y_test_np)\n",
    "\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=10, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=10)\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = (output > 0.5).float() \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return accuracy\n",
    "\n",
    "criterion = nn.BCELoss() \n",
    "\n",
    "optimizers_to_compare = {\n",
    "    \"SGD\": lambda params: optim.SGD(params, lr=0.01),\n",
    "    \"Adam\": lambda params: optim.Adam(params, lr=0.001),\n",
    "    \"SGD_Momentum\": lambda params: optim.SGD(params, lr=0.01, momentum=0.9)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "num_epochs = 20 \n",
    "\n",
    "print(\"Comparing Optimizers:\")\n",
    "for opt_name, opt_func in optimizers_to_compare.items():\n",
    "    print(f\"\\nTraining with {opt_name}...\")\n",
    "    model_pytorch = LLayerNetwork(layer_dimensions)\n",
    "    optimizer = opt_func(model_pytorch.parameters())\n",
    "    \n",
    "    train_model(model_pytorch, train_loader, optimizer, criterion, epochs=num_epochs)\n",
    "    \n",
    "    accuracy = test_model(model_pytorch, test_loader, criterion)\n",
    "    results[opt_name] = accuracy\n",
    "    print(f\"Test Accuracy with {opt_name}: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\n--- Optimizer Comparison Results ---\")\n",
    "for opt_name, acc in results.items():\n",
    "    print(f\"{opt_name}: {acc:.2f}% Test Accuracy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
