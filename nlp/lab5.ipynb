{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c083534",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f7deb3d",
   "metadata": {},
   "source": [
    "# TASK 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41d03d8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing corpus...\n",
      "Building a 3-gram model...\n",
      "Model built. Vocabulary size: 7341\n",
      "Example probability P('is' | ('what', 'is')):  0.0\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import gutenberg\n",
    "from nltk import FreqDist, ngrams\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "\n",
    "class NGramLanguageModel:\n",
    "    def __init__(self, corpus, n=3):\n",
    "        \"\"\"\n",
    "        Ініціалізує та навчає N-грамну модель.\n",
    "        :param corpus: Список речень (списки токенів).\n",
    "        :param n: Порядок N-грам (напр., 2 для біграм, 3 для триграм).\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.vocabulary = set()\n",
    "        self.ngram_counts = defaultdict(FreqDist)\n",
    "        self.context_counts = defaultdict(int)\n",
    "        \n",
    "        print(f\"Building a {n}-gram model...\")\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            sentence_ngrams = ngrams(sentence, n, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "            \n",
    "            for ngram in sentence_ngrams:\n",
    "                self.vocabulary.add(ngram[-1])\n",
    "                \n",
    "                context = ngram[:-1]\n",
    "                target = ngram[-1]\n",
    "                \n",
    "                self.ngram_counts[context][target] += 1\n",
    "                self.context_counts[context] += 1\n",
    "\n",
    "    def get_mle_prob(self, context, target):\n",
    "        \"\"\"\n",
    "        Обчислює ймовірність за методом максимальної правдоподібності (MLE).\n",
    "        P(target | context) = C(context, target) / C(context) \n",
    "        \"\"\"\n",
    "        count = self.ngram_counts[context][target]\n",
    "        total_context_count = self.context_counts[context]\n",
    "        \n",
    "        if total_context_count == 0:\n",
    "            return 0.0\n",
    "        return count / total_context_count\n",
    "\n",
    "print(\"Preparing corpus...\")\n",
    "sents = gutenberg.sents('austen-emma.txt')\n",
    "processed_corpus = [\n",
    "    ['<s>'] * 2 + [word.lower() for word in sent] + ['</s>']\n",
    "    for sent in sents\n",
    "]\n",
    "\n",
    "trigram_model = NGramLanguageModel(processed_corpus, n=3)\n",
    "print(f\"Model built. Vocabulary size: {len(trigram_model.vocabulary)}\")\n",
    "print(\"Example probability P('is' | ('what', 'is')): \", trigram_model.get_mle_prob(('what', 'is'), 'is'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec8335",
   "metadata": {},
   "source": [
    "# TASK 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824b9cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a 2-gram model...\n",
      "Building a 3-gram model...\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "split_ratio = int(len(processed_corpus) * 0.8)\n",
    "train_data = processed_corpus[:split_ratio]\n",
    "test_data = processed_corpus[split_ratio:]\n",
    "\n",
    "bigram_model_base = NGramLanguageModel(train_data, n=2)\n",
    "trigram_model_base = NGramLanguageModel(train_data, n=3)\n",
    "\n",
    "def calculate_perplexity(model, test_sentences):\n",
    "    \"\"\"Обчислює перплексію моделі на тестових даних.\"\"\"\n",
    "    log_prob_sum = 0\n",
    "    word_count = 0\n",
    "\n",
    "    for sentence in test_sentences:\n",
    "        sentence_ngrams = ngrams(sentence, model.n, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "        \n",
    "        for ngram in sentence_ngrams:\n",
    "            context, target = ngram[:-1], ngram[-1]\n",
    "            prob = model.get_smoothed_prob(context, target) \n",
    "            \n",
    "            if prob > 0:\n",
    "                log_prob_sum += math.log(prob)\n",
    "            word_count += 1\n",
    "            \n",
    "    perplexity = math.exp(-log_prob_sum / word_count)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d18ab6",
   "metadata": {},
   "source": [
    "# TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aacf209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building a 2-gram model with backoff...\n",
      "Building a 3-gram model with backoff...\n",
      "\n",
      "--- Model Comparison ---\n",
      "Bigram Model Perplexity: 84.36\n",
      "Trigram Model Perplexity: 96.51\n"
     ]
    }
   ],
   "source": [
    "class NGramLanguageModel:\n",
    "    def __init__(self, corpus, n=3, stupid_backoff_lambda=0.4):\n",
    "        self.n = n\n",
    "        self.stupid_backoff_lambda = stupid_backoff_lambda\n",
    "        self.vocabulary = set()\n",
    "        self.counts = [defaultdict(FreqDist) for _ in range(n)]\n",
    "        self.context_counts = [defaultdict(int) for _ in range(n)]\n",
    "        self.total_words = 0\n",
    "\n",
    "        print(f\"Building a {n}-gram model with backoff...\")\n",
    "        \n",
    "        for sentence in corpus:\n",
    "            self.total_words += len(sentence) - (n - 1) \n",
    "            for i in range(1, n + 1):\n",
    "                sentence_ngrams = ngrams(sentence, i, pad_left=True, pad_right=True, left_pad_symbol='<s>', right_pad_symbol='</s>')\n",
    "                for ngram in sentence_ngrams:\n",
    "                    if i == 1: self.vocabulary.add(ngram[0])\n",
    "                    context, target = ngram[:-1], ngram[-1]\n",
    "                    self.counts[i-1][context][target] += 1\n",
    "                    self.context_counts[i-1][context] += 1\n",
    "\n",
    "    def get_smoothed_prob(self, context, target, current_n=None):\n",
    "        \"\"\"Рекурсивно обчислює ймовірність з використанням Stupid Backoff.\"\"\"\n",
    "        if current_n is None:\n",
    "            current_n = self.n\n",
    "        \n",
    "        if current_n == 1:\n",
    "            return (self.counts[0][()][target] + 1) / (self.total_words + len(self.vocabulary))\n",
    "\n",
    "        context = context[-(current_n-1):] \n",
    "        count = self.counts[current_n-1][context][target]\n",
    "        total_context_count = self.context_counts[current_n-1][context]\n",
    "        \n",
    "        if count > 0 and total_context_count > 0:\n",
    "            return count / total_context_count\n",
    "        else:\n",
    "            return self.stupid_backoff_lambda * self.get_smoothed_prob(context, target, current_n - 1)\n",
    "\n",
    "\n",
    "bigram_model = NGramLanguageModel(train_data, n=2)\n",
    "trigram_model = NGramLanguageModel(train_data, n=3)\n",
    "\n",
    "ppl_bigram = calculate_perplexity(bigram_model, test_data)\n",
    "ppl_trigram = calculate_perplexity(trigram_model, test_data)\n",
    "\n",
    "print(\"\\n--- Model Comparison ---\")\n",
    "print(f\"Bigram Model Perplexity: {ppl_bigram:.2f}\")\n",
    "print(f\"Trigram Model Perplexity: {ppl_trigram:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0f7474",
   "metadata": {},
   "source": [
    "# TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f145ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Sentence Generation (Trigram Model) ---\n",
      "Prompt: 'she was' -> 'she was expecting him every moment .'\n",
      "Prompt: 'he said' -> 'he said he , papa ?'\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def generate_sentence(model, prompt, max_length=20):\n",
    "    \"\"\"\n",
    "    Генерує продовження речення для заданого промпту.\n",
    "    \"\"\"\n",
    "    sentence = prompt.lower().split()\n",
    "    context = tuple(['<s>'] * (model.n - 1 - len(sentence)) + sentence)\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        candidates = model.vocabulary\n",
    "        probabilities = [model.get_smoothed_prob(context, candidate) for candidate in candidates]\n",
    "        \n",
    "        next_word = random.choices(list(candidates), weights=probabilities, k=1)[0]\n",
    "        \n",
    "        if next_word == '</s>':\n",
    "            break\n",
    "        \n",
    "        sentence.append(next_word)\n",
    "        context = tuple(sentence[-(model.n - 1):])\n",
    "\n",
    "    return ' '.join(sentence)\n",
    "\n",
    "print(\"\\n--- Sentence Generation (Trigram Model) ---\")\n",
    "prompt1 = \"she was\"\n",
    "generated_text1 = generate_sentence(trigram_model, prompt1)\n",
    "print(f\"Prompt: '{prompt1}' -> '{generated_text1}'\")\n",
    "\n",
    "prompt2 = \"he said\"\n",
    "generated_text2 = generate_sentence(trigram_model, prompt2)\n",
    "print(f\"Prompt: '{prompt2}' -> '{generated_text2}'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
