{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72f6113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import twitter_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb11277",
   "metadata": {},
   "source": [
    "# TASK 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f443dcd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оригінальні токени: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "Лематизовані токени: ['@groovinshawn', 'they', 'be', 'rechargeable', 'and', 'it', 'normally', 'come', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import twitter_samples\n",
    "\n",
    "tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "tokens = tweet_tokens[50]\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatized_sentence = []\n",
    "    # CODE_START\n",
    "    for token, tag in pos_tag(tokens):\n",
    "        # Перетворення тегів PoS\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n' # Noun\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v' # Verb\n",
    "        else:\n",
    "            pos = 'a' # Adjective (для решти випадків)\n",
    "        \n",
    "        # Лематизація токена з відповідним тегом\n",
    "        lemmatized_token = lemmatizer.lemmatize(token, pos)\n",
    "        lemmatized_sentence.append(lemmatized_token)\n",
    "    # CODE_END\n",
    "    return lemmatized_sentence\n",
    "\n",
    "# Тестування функції\n",
    "print(\"Оригінальні токени:\", tokens)\n",
    "print(\"Лематизовані токени:\", lemmatize_sentence(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fdf5dac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Before: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "After: ['rechargeable', 'normally', 'come', 'charger', 'u', 'buy', ':)']\n"
     ]
    }
   ],
   "source": [
    "import re, string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "def process_tokens(tweet_tokens):\n",
    "    cleaned_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # CODE_START\n",
    "        # Видалення URL та згадок\n",
    "        token = re.sub('(@[A-Za-z0-9_]+)|(https?://[A-Za-z0-9./]+)', '', token)\n",
    "\n",
    "        # Перевірка на стоп-слова та пунктуацію після видалення regex\n",
    "        if token and token.lower() not in stop_words and token not in string.punctuation:\n",
    "            # Перетворення тегів PoS\n",
    "            if tag.startswith('NN'):\n",
    "                pos = 'n'\n",
    "            elif tag.startswith('VB'):\n",
    "                pos = 'v'\n",
    "            else:\n",
    "                pos = 'a'\n",
    "\n",
    "            # Лематизація та приведення до нижнього регістру\n",
    "            cleaned_token = lemmatizer.lemmatize(token.lower(), pos)\n",
    "            cleaned_tokens.append(cleaned_token)\n",
    "        # CODE_END\n",
    "    return cleaned_tokens\n",
    "\n",
    "# Тестування функції\n",
    "print(\"\\nBefore:\", tweet_tokens[50])\n",
    "print(\"After:\", process_tokens(tweet_tokens[50]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37927545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Оригінальний твіт 500: ['Dang', 'that', 'is', 'some', 'rad', '@AbzuGame', '#fanart', '!', ':D', 'https://t.co/bI8k8tb9ht']\n",
      "Очищений твіт 500: ['dang', 'rad', '#fanart', ':d']\n"
     ]
    }
   ],
   "source": [
    "# CODE_START\n",
    "positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "positive_cleaned_tokens_list = [process_tokens(tokens) for tokens in positive_tweet_tokens]\n",
    "negative_cleaned_tokens_list = [process_tokens(tokens) for tokens in negative_tweet_tokens]\n",
    "# CODE_END\n",
    "\n",
    "# Перевірка результату\n",
    "print(\"\\nОригінальний твіт 500:\", positive_tweet_tokens[500])\n",
    "print(\"Очищений твіт 500:\", positive_cleaned_tokens_list[500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b827e79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "10 найпоширеніших слів у позитивних твітах:\n",
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 383), ('follow', 362), ('love', 336), ('...', 290), ('good', 283), ('get', 269), ('thank', 258)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import FreqDist\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    # CODE_START\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "    # CODE_END\n",
    "\n",
    "all_pos_words = list(get_all_words(positive_cleaned_tokens_list))\n",
    "\n",
    "# CODE_START\n",
    "# Виконання частотного аналізу\n",
    "freq_dist_pos = FreqDist(all_pos_words)\n",
    "print(\"\\n10 найпоширеніших слів у позитивних твітах:\")\n",
    "print(freq_dist_pos.most_common(10))\n",
    "# CODE_END"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c187f751",
   "metadata": {},
   "source": [
    "# TASK 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a96aeb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Тест видалення хештегів:\n",
      "Оригінал: ['I', 'love', '#NLP', 'and', '#AI', '!', 'It', 'is', 'awesome', '.']\n",
      "Після очищення: ['love', 'awesome']\n"
     ]
    }
   ],
   "source": [
    "def process_tokens_no_hashtags(tweet_tokens):\n",
    "    cleaned_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        # Додано патерн для хештегів: (#[A-Za-z0-9_]+)\n",
    "        token = re.sub('(@[A-Za-z0-9_]+)|(https?://[A-Za-z0-9./]+)|(#[A-Za-z0-9_]+)', '', token)\n",
    "\n",
    "        if token and token.lower() not in stop_words and token not in string.punctuation:\n",
    "            if tag.startswith('NN'):\n",
    "                pos = 'n'\n",
    "            elif tag.startswith('VB'):\n",
    "                pos = 'v'\n",
    "            else:\n",
    "                pos = 'a'\n",
    "            cleaned_token = lemmatizer.lemmatize(token.lower(), pos)\n",
    "            cleaned_tokens.append(cleaned_token)\n",
    "            \n",
    "    return cleaned_tokens\n",
    "\n",
    "# Тестовий твіт з хештегом\n",
    "test_tokens_hashtag = ['I', 'love', '#NLP', 'and', '#AI', '!', 'It', 'is', 'awesome', '.']\n",
    "print(\"\\nТест видалення хештегів:\")\n",
    "print(\"Оригінал:\", test_tokens_hashtag)\n",
    "print(\"Після очищення:\", process_tokens_no_hashtags(test_tokens_hashtag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93482c42",
   "metadata": {},
   "source": [
    "# TASK 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ff8d28e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Тест заміни на синсети:\n",
      "Оригінал: ['@groovinshawn', 'they', 'are', 'rechargeable', 'and', 'it', 'normally', 'comes', 'with', 'a', 'charger', 'when', 'u', 'buy', 'it', ':)']\n",
      "Після обробки з синсетами: ['rechargeable.s.01', 'come.v.01', 'charger.n.01', 'u.s.01', 'buy.v.01']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def process_tokens_synsets(tweet_tokens):\n",
    "    cleaned_tokens = []\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('(@[A-Za-z0-9_]+)|(https?://[A-Za-z0-9./]+)', '', token)\n",
    "\n",
    "        if token and token.lower() not in stop_words and token not in string.punctuation:\n",
    "            # Конвертація тега\n",
    "            if tag.startswith('NN'):\n",
    "                pos = 'n'\n",
    "            elif tag.startswith('VB'):\n",
    "                pos = 'v'\n",
    "            else:\n",
    "                pos = 'a'\n",
    "            \n",
    "            # Пошук синсетів\n",
    "            synsets = wn.synsets(token.lower(), pos)\n",
    "            \n",
    "            # Якщо синсети знайдено, беремо ім'я першого (найпоширенішого)\n",
    "            if synsets:\n",
    "                cleaned_tokens.append(synsets[0].name())\n",
    "            # Інакше, можемо додати сам токен (опціонально)\n",
    "            # else:\n",
    "            #     cleaned_tokens.append(token.lower())\n",
    "                \n",
    "    return cleaned_tokens\n",
    "    \n",
    "# Тестування функції\n",
    "print(\"\\nТест заміни на синсети:\")\n",
    "print(\"Оригінал:\", tweet_tokens[50])\n",
    "print(\"Після обробки з синсетами:\", process_tokens_synsets(tweet_tokens[50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a0d3a",
   "metadata": {},
   "source": [
    "# TASK 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7546d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Семантична відстань між 'car' та 'automobile': 0\n",
      "Семантична відстань між 'car' та 'boat': 7\n",
      "Семантична відстань між 'tree' та 'cat': 12\n"
     ]
    }
   ],
   "source": [
    "def semantic_distance(word1, word2):\n",
    "    \"\"\"\n",
    "    Обчислює семантичну відстань між двома словами.\n",
    "    Відстань = відстань(слово1 -> спільний_предок) + відстань(слово2 -> спільний_предок)\n",
    "    \"\"\"\n",
    "    # Беремо перший (найпоширеніший) синсет для кожного слова\n",
    "    syn1 = wn.synsets(word1)\n",
    "    syn2 = wn.synsets(word2)\n",
    "    \n",
    "    if not syn1 or not syn2:\n",
    "        print(f\"Одне зі слів ('{word1}' або '{word2}') не знайдено в WordNet.\")\n",
    "        return None\n",
    "        \n",
    "    s1 = syn1[0]\n",
    "    s2 = syn2[0]\n",
    "\n",
    "    # Знаходимо найближчий спільний гіперонім\n",
    "    common_hypernyms = s1.lowest_common_hypernyms(s2)\n",
    "    \n",
    "    if not common_hypernyms:\n",
    "        print(f\"Не знайдено спільних гіперонімів для '{word1}' та '{word2}'.\")\n",
    "        return None\n",
    "        \n",
    "    common = common_hypernyms[0]\n",
    "    \n",
    "    # Обчислюємо відстань від кожного синсета до спільного предка\n",
    "    dist1 = s1.shortest_path_distance(common)\n",
    "    dist2 = s2.shortest_path_distance(common)\n",
    "    \n",
    "    return dist1 + dist2\n",
    "\n",
    "# Приклади використання\n",
    "dist_car_auto = semantic_distance(\"car\", \"automobile\")\n",
    "print(f\"\\nСемантична відстань між 'car' та 'automobile': {dist_car_auto}\") # Очікується 0, бо це синоніми в одному синсеті\n",
    "\n",
    "dist_car_boat = semantic_distance(\"car\", \"boat\")\n",
    "print(f\"Семантична відстань між 'car' та 'boat': {dist_car_boat}\")\n",
    "\n",
    "dist_tree_cat = semantic_distance(\"tree\", \"cat\")\n",
    "print(f\"Семантична відстань між 'tree' та 'cat': {dist_tree_cat}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
